PS C:\Users\coolb> & C:/Users/coolb/anaconda3/python.exe c:/Users/coolb/Desktop/SarBERT/RoBERTa_Model.py
C:\Users\coolb\anaconda3\Lib\site-packages\transformers\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.opnstead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1/4
----------
Training Roberta: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2460/2460 [12:32<00:00, 
Train loss 0.1963108310468374 accuracy 0.9204146973624029
Evaluating Roberta: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1055/1055 [01:47<00:00, 
Val   loss 0.1614095492245435 accuracy 0.9373925416493745

Epoch 2/4
----------
Training Roberta: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2460/2460 [12:32<00:00, 
Train loss 0.09624797561744462 accuracy 0.9621131270010673
Evaluating Roberta: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1055/1055 [01:47<00:00, 
Val   loss 0.13698242041938916 accuracy 0.9455741981383767

Epoch 3/4
----------
Training Roberta: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2460/2460 [12:32<00:00, 
Train loss 0.057018681342902354 accuracy 0.9793159526350562
Evaluating Roberta: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1055/1055 [01:47<00:00, 
Val   loss 0.2020042125513596 accuracy 0.9331831386731487

Epoch 4/4
----------
Training Roberta: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2460/2460 [12:32<00:00, 
Train loss 0.041675101798372034 accuracy 0.9859480611881893
Evaluating Roberta: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1055/1055 [01:47<00:00, 
Val   loss 0.2138315863067602 accuracy 0.9401790478449042
Testing Roberta: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1055/1055 [01:41<00:00, 10.37it/s]
Roberta Classification Report:
              precision    recall  f1-score   support

    negative       0.75      0.76      0.76       381
     neutral       0.97      0.94      0.96     11625
    positive       0.89      0.95      0.92      4861

    accuracy                           0.94     16867
   macro avg       0.87      0.88      0.88     16867
weighted avg       0.94      0.94      0.94     16867

Accuracy: 0.9401790478449042
Precision: 0.9420325755929277
Recall: 0.9401790478449042
F1-Score: 0.9406147595216936
PS C:\Users\coolb>